{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qe9ZhUB4CV1G",
        "outputId": "f4b3c27c-2b31-4c06-9112-495ea33efc86"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet18-baseline</strong> at: <a href='https://wandb.ai/ir2023/tiny-imagenet-assignment/runs/c0e1is30' target=\"_blank\">https://wandb.ai/ir2023/tiny-imagenet-assignment/runs/c0e1is30</a><br> View project at: <a href='https://wandb.ai/ir2023/tiny-imagenet-assignment' target=\"_blank\">https://wandb.ai/ir2023/tiny-imagenet-assignment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251106_164135-c0e1is30/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251106_165432-2olosu8e</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ir2023/tiny-imagenet-assignment/runs/2olosu8e' target=\"_blank\">resnet18-baseline</a></strong> to <a href='https://wandb.ai/ir2023/tiny-imagenet-assignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ir2023/tiny-imagenet-assignment' target=\"_blank\">https://wandb.ai/ir2023/tiny-imagenet-assignment</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ir2023/tiny-imagenet-assignment/runs/2olosu8e' target=\"_blank\">https://wandb.ai/ir2023/tiny-imagenet-assignment/runs/2olosu8e</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading datasets...\n",
            "Found 200 classes in training set\n",
            "Val set has class folder structure with 200 classes\n",
            "Train size: 20000\n",
            "Val size: 2000\n",
            "Test size: 2000\n",
            "Initializing resnet18...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 194MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [32:58<00:00,  6.32s/it, loss=4.49, acc=5.63]\n",
            "Evaluating: 100%|██████████| 32/32 [03:17<00:00,  6.18s/it, loss=4.63, acc=9.75]\n",
            "Evaluating: 100%|██████████| 32/32 [03:07<00:00,  5.87s/it, loss=7.34, acc=0.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 4.7263, Train Acc: 5.63%\n",
            "Val Loss: 4.2179, Val Acc: 9.75%\n",
            "Test Loss: 7.8573, Test Acc: 0.30%\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [02:07<00:00,  2.45it/s, loss=3.85, acc=12.3]\n",
            "Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.36it/s, loss=4.31, acc=15]\n",
            "Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.83it/s, loss=7.14, acc=1.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 4.0838, Train Acc: 12.34%\n",
            "Val Loss: 3.8575, Val Acc: 15.00%\n",
            "Test Loss: 7.5138, Test Acc: 1.30%\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [02:08<00:00,  2.44it/s, loss=3.52, acc=16.9]\n",
            "Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.35it/s, loss=4.98, acc=20.6]\n",
            "Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.83it/s, loss=9.2, acc=0.55]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.7756, Train Acc: 16.88%\n",
            "Val Loss: 3.4977, Val Acc: 20.60%\n",
            "Test Loss: 9.8222, Test Acc: 0.55%\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [02:08<00:00,  2.44it/s, loss=4.04, acc=20.5]\n",
            "Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.32it/s, loss=3.58, acc=24.4]\n",
            "Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.64it/s, loss=9.14, acc=0.25]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.5412, Train Acc: 20.55%\n",
            "Val Loss: 3.3118, Val Acc: 24.45%\n",
            "Test Loss: 9.6236, Test Acc: 0.25%\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [02:08<00:00,  2.44it/s, loss=3.14, acc=23.6]\n",
            "Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.32it/s, loss=3.68, acc=27.6]\n",
            "Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.29it/s, loss=10.4, acc=0.35]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.3562, Train Acc: 23.56%\n",
            "Val Loss: 3.1178, Val Acc: 27.65%\n",
            "Test Loss: 10.6641, Test Acc: 0.35%\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [02:08<00:00,  2.44it/s, loss=2.45, acc=33.4]\n",
            "Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.49it/s, loss=3.99, acc=37.7]\n",
            "Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.34it/s, loss=11, acc=0.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.8568, Train Acc: 33.45%\n",
            "Val Loss: 2.6249, Val Acc: 37.70%\n",
            "Test Loss: 11.5770, Test Acc: 0.30%\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [02:09<00:00,  2.42it/s, loss=3.7, acc=37.2]\n",
            "Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.79it/s, loss=3.88, acc=39]\n",
            "Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.28it/s, loss=11.2, acc=0.35]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.6658, Train Acc: 37.25%\n",
            "Val Loss: 2.5563, Val Acc: 38.95%\n",
            "Test Loss: 11.7989, Test Acc: 0.35%\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [02:08<00:00,  2.44it/s, loss=2.1, acc=38.5]\n",
            "Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.59it/s, loss=3.44, acc=38.9]\n",
            "Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.49it/s, loss=11.7, acc=0.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.5910, Train Acc: 38.53%\n",
            "Val Loss: 2.5128, Val Acc: 38.90%\n",
            "Test Loss: 12.2037, Test Acc: 0.30%\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [02:08<00:00,  2.44it/s, loss=2.74, acc=39.9]\n",
            "Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.49it/s, loss=3.61, acc=41]\n",
            "Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.66it/s, loss=10.9, acc=0.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.5029, Train Acc: 39.94%\n",
            "Val Loss: 2.4639, Val Acc: 40.95%\n",
            "Test Loss: 11.3821, Test Acc: 0.40%\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [02:09<00:00,  2.43it/s, loss=2.38, acc=41.1]\n",
            "Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.32it/s, loss=3.64, acc=40.9]\n",
            "Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.88it/s, loss=10.9, acc=0.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.4512, Train Acc: 41.08%\n",
            "Val Loss: 2.4501, Val Acc: 40.90%\n",
            "Test Loss: 11.5431, Test Acc: 0.40%\n",
            "\n",
            "Training completed! Best validation accuracy: 40.95%\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning_rate</td><td>████▂▂▂▂▂▁</td></tr><tr><td>test_acc</td><td>▁█▃▁▂▁▂▁▂▂</td></tr><tr><td>test_loss</td><td>▂▁▄▄▆▇▇█▇▇</td></tr><tr><td>train_acc</td><td>▁▂▃▄▅▆▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▃▄▅▇████</td></tr><tr><td>val_loss</td><td>█▇▅▄▄▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>test_acc</td><td>0.4</td></tr><tr><td>test_loss</td><td>11.54309</td></tr><tr><td>train_acc</td><td>41.075</td></tr><tr><td>train_loss</td><td>2.45125</td></tr><tr><td>val_acc</td><td>40.9</td></tr><tr><td>val_loss</td><td>2.45011</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet18-baseline</strong> at: <a href='https://wandb.ai/ir2023/tiny-imagenet-assignment/runs/2olosu8e' target=\"_blank\">https://wandb.ai/ir2023/tiny-imagenet-assignment/runs/2olosu8e</a><br> View project at: <a href='https://wandb.ai/ir2023/tiny-imagenet-assignment' target=\"_blank\">https://wandb.ai/ir2023/tiny-imagenet-assignment</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251106_165432-2olosu8e/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import wandb\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "class Config:\n",
        "    # Dataset paths\n",
        "    DATA_DIR = \"/tiny-imagenet-200\"\n",
        "\n",
        "    # Reduce dataset size (set to None to use full dataset)\n",
        "    TRAIN_SUBSET_SIZE = 20000 \n",
        "    VAL_SUBSET_SIZE = 2000    \n",
        "    TEST_SUBSET_SIZE = 2000  \n",
        "\n",
        "    # Training hyperparameters\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 10\n",
        "    LEARNING_RATE = 0.001\n",
        "    NUM_WORKERS = 4\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Model\n",
        "    MODEL_NAME = \"resnet18\"  \n",
        "    NUM_CLASSES = 200\n",
        "    PRETRAINED = True\n",
        "\n",
        "    # W&B\n",
        "    WANDB_PROJECT = \"tiny-imagenet-assignment\"\n",
        "    WANDB_RUN_NAME = \"resnet18-baseline\"\n",
        "\n",
        "class TinyImageNetDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: Root directory of tiny-imagenet-200\n",
        "            split: 'train', 'val', or 'test'\n",
        "            transform: Transformations to apply\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        self.class_to_idx = {}\n",
        "\n",
        "        if split == 'train':\n",
        "            self._load_train_data()\n",
        "        elif split == 'val':\n",
        "            self._load_val_data()\n",
        "        elif split == 'test':\n",
        "            self._load_test_data()\n",
        "\n",
        "    def _load_train_data(self):\n",
        "        train_dir = os.path.join(self.root_dir, 'train')\n",
        "\n",
        "        # Check if train directory exists\n",
        "        if not os.path.exists(train_dir):\n",
        "            raise FileNotFoundError(f\"Train directory not found: {train_dir}\")\n",
        "\n",
        "        classes = sorted([d for d in os.listdir(train_dir)\n",
        "                         if os.path.isdir(os.path.join(train_dir, d))])\n",
        "\n",
        "        print(f\"Found {len(classes)} classes in training set\")\n",
        "\n",
        "        for idx, class_name in enumerate(classes):\n",
        "            self.class_to_idx[class_name] = idx\n",
        "            class_dir = os.path.join(train_dir, class_name, 'images')\n",
        "\n",
        "            # Try without 'images' subfolder if it doesn't exist\n",
        "            if not os.path.exists(class_dir):\n",
        "                class_dir = os.path.join(train_dir, class_name)\n",
        "\n",
        "            if os.path.exists(class_dir):\n",
        "                for img_name in os.listdir(class_dir):\n",
        "                    if img_name.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
        "                        img_path = os.path.join(class_dir, img_name)\n",
        "                        self.samples.append((img_path, idx))\n",
        "\n",
        "    def _load_val_data(self):\n",
        "        val_dir = os.path.join(self.root_dir, 'val')\n",
        "\n",
        "        # Check if val directory exists\n",
        "        if not os.path.exists(val_dir):\n",
        "            raise FileNotFoundError(f\"Val directory not found: {val_dir}\")\n",
        "\n",
        "        # Load class mapping from train\n",
        "        train_dir = os.path.join(self.root_dir, 'train')\n",
        "        classes = sorted([d for d in os.listdir(train_dir)\n",
        "                         if os.path.isdir(os.path.join(train_dir, d))])\n",
        "        for idx, class_name in enumerate(classes):\n",
        "            self.class_to_idx[class_name] = idx\n",
        "\n",
        "        # Check if val has the same structure as train (class folders)\n",
        "        val_classes = [d for d in os.listdir(val_dir)\n",
        "                      if os.path.isdir(os.path.join(val_dir, d)) and d in self.class_to_idx]\n",
        "\n",
        "        if len(val_classes) > 0:\n",
        "            # Val has class folder structure (like train)\n",
        "            print(f\"Val set has class folder structure with {len(val_classes)} classes\")\n",
        "            for class_name in val_classes:\n",
        "                class_idx = self.class_to_idx[class_name]\n",
        "                class_dir = os.path.join(val_dir, class_name, 'images')\n",
        "\n",
        "                # Try without 'images' subfolder if it doesn't exist\n",
        "                if not os.path.exists(class_dir):\n",
        "                    class_dir = os.path.join(val_dir, class_name)\n",
        "\n",
        "                if os.path.exists(class_dir):\n",
        "                    for img_name in os.listdir(class_dir):\n",
        "                        if img_name.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
        "                            img_path = os.path.join(class_dir, img_name)\n",
        "                            self.samples.append((img_path, class_idx))\n",
        "        else:\n",
        "            # Try val_annotations.txt format\n",
        "            val_annotations = os.path.join(val_dir, 'val_annotations.txt')\n",
        "\n",
        "            if os.path.exists(val_annotations):\n",
        "                print(\"Using val_annotations.txt\")\n",
        "                with open(val_annotations, 'r') as f:\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split('\\t')\n",
        "                        img_name = parts[0]\n",
        "                        class_name = parts[1]\n",
        "                        img_path = os.path.join(val_dir, 'images', img_name)\n",
        "\n",
        "                        if not os.path.exists(img_path):\n",
        "                            img_path = os.path.join(val_dir, img_name)\n",
        "\n",
        "                        if os.path.exists(img_path) and class_name in self.class_to_idx:\n",
        "                            self.samples.append((img_path, self.class_to_idx[class_name]))\n",
        "            else:\n",
        "                # Just load all images from val directory without labels\n",
        "                print(\"Warning: No class structure or annotations found. Loading images without proper labels.\")\n",
        "                print(\"Using train class mapping for available val class folders\")\n",
        "\n",
        "                # Try to load from images subfolder or directly from val\n",
        "                images_dir = os.path.join(val_dir, 'images')\n",
        "                if not os.path.exists(images_dir):\n",
        "                    images_dir = val_dir\n",
        "\n",
        "                for img_name in os.listdir(images_dir):\n",
        "                    if img_name.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
        "                        img_path = os.path.join(images_dir, img_name)\n",
        "                        # Assign label 0 as placeholder (not ideal but allows loading)\n",
        "                        self.samples.append((img_path, 0))\n",
        "\n",
        "    def _load_test_data(self):\n",
        "        test_dir = os.path.join(self.root_dir, 'test')\n",
        "\n",
        "        # Check if test directory exists\n",
        "        if not os.path.exists(test_dir):\n",
        "            print(\"Test directory not found, using val as test\")\n",
        "            self._load_val_data()\n",
        "            return\n",
        "\n",
        "        # Load class mapping from train\n",
        "        train_dir = os.path.join(self.root_dir, 'train')\n",
        "        classes = sorted([d for d in os.listdir(train_dir)\n",
        "                         if os.path.isdir(os.path.join(train_dir, d))])\n",
        "        for idx, class_name in enumerate(classes):\n",
        "            self.class_to_idx[class_name] = idx\n",
        "\n",
        "        # Check if test has class folder structure\n",
        "        test_classes = [d for d in os.listdir(test_dir)\n",
        "                       if os.path.isdir(os.path.join(test_dir, d)) and d in self.class_to_idx]\n",
        "\n",
        "        if len(test_classes) > 0:\n",
        "            # Test has class folder structure\n",
        "            print(f\"Test set has class folder structure with {len(test_classes)} classes\")\n",
        "            for class_name in test_classes:\n",
        "                class_idx = self.class_to_idx[class_name]\n",
        "                class_dir = os.path.join(test_dir, class_name, 'images')\n",
        "\n",
        "                if not os.path.exists(class_dir):\n",
        "                    class_dir = os.path.join(test_dir, class_name)\n",
        "\n",
        "                if os.path.exists(class_dir):\n",
        "                    for img_name in os.listdir(class_dir):\n",
        "                        if img_name.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
        "                            img_path = os.path.join(class_dir, img_name)\n",
        "                            self.samples.append((img_path, class_idx))\n",
        "        else:\n",
        "            # Load images without labels (typical test set)\n",
        "            images_dir = os.path.join(test_dir, 'images')\n",
        "            if not os.path.exists(images_dir):\n",
        "                images_dir = test_dir\n",
        "\n",
        "            for img_name in os.listdir(images_dir):\n",
        "                if img_name.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
        "                    img_path = os.path.join(images_dir, img_name)\n",
        "                    self.samples.append((img_path, 0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# ===========================\n",
        "# Data Transforms\n",
        "# ===========================\n",
        "def get_transforms():\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_test_transform\n",
        "\n",
        "# ===========================\n",
        "# Model Setup\n",
        "# ===========================\n",
        "def get_model(model_name, num_classes, pretrained=True):\n",
        "    if model_name == 'resnet18':\n",
        "        model = models.resnet18(pretrained=pretrained)\n",
        "    elif model_name == 'resnet34':\n",
        "        model = models.resnet34(pretrained=pretrained)\n",
        "    elif model_name == 'resnet50':\n",
        "        model = models.resnet50(pretrained=pretrained)\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} not supported\")\n",
        "\n",
        "    # Modify final layer for Tiny ImageNet (200 classes)\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "# ===========================\n",
        "# Training Functions\n",
        "# ===========================\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc='Training')\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(dataloader, desc='Evaluating')\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# ===========================\n",
        "# Main Training Function\n",
        "# ===========================\n",
        "def main():\n",
        "    config = Config()\n",
        "\n",
        "    # Initialize W&B\n",
        "    wandb.init(\n",
        "        project=config.WANDB_PROJECT,\n",
        "        name=config.WANDB_RUN_NAME,\n",
        "        config={\n",
        "            \"model\": config.MODEL_NAME,\n",
        "            \"batch_size\": config.BATCH_SIZE,\n",
        "            \"epochs\": config.NUM_EPOCHS,\n",
        "            \"learning_rate\": config.LEARNING_RATE,\n",
        "            \"pretrained\": config.PRETRAINED,\n",
        "            \"train_subset_size\": config.TRAIN_SUBSET_SIZE,\n",
        "            \"val_subset_size\": config.VAL_SUBSET_SIZE,\n",
        "            \"test_subset_size\": config.TEST_SUBSET_SIZE,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"Using device: {config.DEVICE}\")\n",
        "\n",
        "    # Get transforms\n",
        "    train_transform, val_test_transform = get_transforms()\n",
        "\n",
        "    # Load datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset = TinyImageNetDataset(\n",
        "        config.DATA_DIR, split='train', transform=train_transform\n",
        "    )\n",
        "    val_dataset = TinyImageNetDataset(\n",
        "        config.DATA_DIR, split='val', transform=val_test_transform\n",
        "    )\n",
        "    test_dataset = TinyImageNetDataset(\n",
        "        config.DATA_DIR, split='test', transform=val_test_transform\n",
        "    )\n",
        "\n",
        "    # Create subsets to reduce dataset size\n",
        "    if config.TRAIN_SUBSET_SIZE:\n",
        "        train_indices = random.sample(range(len(train_dataset)),\n",
        "                                     min(config.TRAIN_SUBSET_SIZE, len(train_dataset)))\n",
        "        train_dataset = Subset(train_dataset, train_indices)\n",
        "\n",
        "    if config.VAL_SUBSET_SIZE:\n",
        "        val_indices = random.sample(range(len(val_dataset)),\n",
        "                                   min(config.VAL_SUBSET_SIZE, len(val_dataset)))\n",
        "        val_dataset = Subset(val_dataset, val_indices)\n",
        "\n",
        "    if config.TEST_SUBSET_SIZE:\n",
        "        test_indices = random.sample(range(len(test_dataset)),\n",
        "                                    min(config.TEST_SUBSET_SIZE, len(test_dataset)))\n",
        "        test_dataset = Subset(test_dataset, test_indices)\n",
        "\n",
        "    print(f\"Train size: {len(train_dataset)}\")\n",
        "    print(f\"Val size: {len(val_dataset)}\")\n",
        "    print(f\"Test size: {len(test_dataset)}\")\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=config.BATCH_SIZE,\n",
        "        shuffle=True, num_workers=config.NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=config.BATCH_SIZE,\n",
        "        shuffle=False, num_workers=config.NUM_WORKERS\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=config.BATCH_SIZE,\n",
        "        shuffle=False, num_workers=config.NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    print(f\"Initializing {config.MODEL_NAME}...\")\n",
        "    model = get_model(config.MODEL_NAME, config.NUM_CLASSES, config.PRETRAINED)\n",
        "    model = model.to(config.DEVICE)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, config.DEVICE\n",
        "        )\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc = evaluate(\n",
        "            model, val_loader, criterion, config.DEVICE\n",
        "        )\n",
        "\n",
        "        # Test\n",
        "        test_loss, test_acc = evaluate(\n",
        "            model, test_loader, criterion, config.DEVICE\n",
        "        )\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Log to W&B\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_acc\": val_acc,\n",
        "            \"test_loss\": test_loss,\n",
        "            \"test_acc\": test_acc,\n",
        "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "        })\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "            }, 'best_model.pth')\n",
        "\n",
        "            # Log model to W&B\n",
        "            wandb.save('best_model.pth')\n",
        "\n",
        "    print(f\"\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # Log final model as artifact\n",
        "    artifact = wandb.Artifact(\n",
        "        name=f\"{config.MODEL_NAME}-tiny-imagenet\",\n",
        "        type=\"model\",\n",
        "        description=f\"Trained {config.MODEL_NAME} on Tiny ImageNet\"\n",
        "    )\n",
        "    artifact.add_file('best_model.pth')\n",
        "    wandb.log_artifact(artifact)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
